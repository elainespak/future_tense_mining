{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from contextlib import redirect_stderr\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "df_us_10k = pd.read_csv('D:\\\\data\\\\finance\\\\financial_statements\\\\us\\\\SEC-preprocessed-download\\\\10k_2009_2019.csv', encoding='utf-8')\n",
    "df_us_10k = df_us_10k[['adsh', 'cik', 'gvkey', 'form', 'filed', 'co_conm']].drop_duplicates()\n",
    "df_kospi_10k_1115 = pd.read_csv('D:\\\\data\\\\finance\\\\financial_statements\\\\kr\\\\kospi200_2011_2015_translated.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "error on 0001274494-10-000029\n",
      "error on 0000768251-10-000008\n",
      "error on 0001274494-10-000039\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "error on 0000023217-13-000005\n",
      "10000\n",
      "error on 0001123360-13-000031\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n"
     ]
    }
   ],
   "source": [
    "def will_occurences(string):\n",
    "    return len(re.findall('(?!A\\s*|The\\s*)(will)', string))\n",
    "\n",
    "def file_to_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "          data = f.read()\n",
    "    return data\n",
    "\n",
    "path = 'D:\\\\data\\\\finance\\\\financial_statements\\\\us\\\\EDGAR-raw\\\\'\n",
    "grammatic_tokens = ['will', 'shall', 'going_to', 'expect_to', 'expected_to', 'expecting_to']\n",
    "gramm_tf = {token: [] for token in grammatic_tokens}\n",
    "err = []\n",
    "err_flag = False\n",
    "for idx, row in df_us_10k.iterrows():\n",
    "    try:\n",
    "        string = file_to_string(path + row.adsh + '.txt')\n",
    "    except:\n",
    "        print('error on '+ str(row.adsh))\n",
    "        err += [row.adsh]\n",
    "        err_flag = True\n",
    "    for token in gramm_tf.keys():\n",
    "        if err_flag:\n",
    "            gramm_tf[token] += [None]\n",
    "        else:\n",
    "            if token == 'will':\n",
    "                gramm_tf[token] += [will_occurences(string)]\n",
    "            else:\n",
    "                gramm_tf[token] += [string.count(token)]\n",
    "    err_flag = False\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "\n",
    "for token in gramm_tf.keys():\n",
    "    df_us_10k[token] = gramm_tf[token]\n",
    "\n",
    "df_us_10k.to_csv('10k_grammatical_index.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf with skelarn - \n",
    "vectorizer = TfidfVectorizer(vocabulary={'will':0, 'shall':1})\n",
    "X = vectorizer.fit_transform(df_i_want['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_tf = pd.DataFrame(columns=('adsh', 'gvkey', 'cik', 'filed'))\n",
    "df_us_tf[['adsh', 'gvkey', 'cik', 'filed']] = df_us_10k[['adsh', 'gvkey', 'cik', 'filed']].dropna()\n",
    "df_us_tf['will'] = df_us_10k['text'].dropna().apply(will_occurences)\n",
    "df_us_tf['shall'] = df_us_10k['text'].dropna().str.count('shall')\n",
    "df_us_tf['going_to'] = df_us_10k['text'].dropna().str.count('going to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_tf['expect_to'] = df_us_10k['text'].dropna().str.count('expect to')\n",
    "df_us_tf['expected_to'] = df_us_10k['text'].dropna().str.count('expected to')\n",
    "df_us_tf['expecting_to'] = df_us_10k['text'].dropna().str.count('expecting to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "will            18.653786\n",
       "shall            0.150051\n",
       "going_to         0.004285\n",
       "expect_to        0.594213\n",
       "expecting_to     0.002124\n",
       "expected_to      3.090782\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_tf[['will', 'shall', 'going_to', 'expect_to', 'expecting_to', 'expected_to']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adsh</th>\n",
       "      <th>cik</th>\n",
       "      <th>filed</th>\n",
       "      <th>will</th>\n",
       "      <th>shall</th>\n",
       "      <th>going_to</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       adsh  cik  filed  will  shall  going_to\n",
       "filed                                         \n",
       "2009     25   25     25    25     25        25\n",
       "2010     51   51     51    51     51        51\n",
       "2011     38   38     38    38     38        38\n",
       "2012     47   47     47    47     47        47\n",
       "2013     59   59     59    59     59        59\n",
       "2014     36   36     36    36     36        36\n",
       "2015     18   18     18    18     18        18\n",
       "2016     27   27     27    27     27        27\n",
       "2017     28   28     28    28     28        28\n",
       "2018     14   14     14    14     14        14\n",
       "2019     12   12     12    12     12        12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_tf.filed = pd.to_datetime(df_us_tf.filed, format='%Y%m%d')\n",
    "df_us_tf[df_us_tf.will==0].groupby(df_us_tf.filed.dt.year).count()\n",
    "#will이 1개도 등장하지 않는 애들이 약 30~40개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KOSPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kospi_10k_1115[df_kospi_10k_1115.Translated.isnull()]\n",
    "df_kr_tf = pd.DataFrame(columns=['file'])\n",
    "df_kr_tf[['file']] = df_kospi_10k_1115[['File']].dropna()\n",
    "df_kr_tf['will'] = df_kospi_10k_1115['Translated'].dropna().apply(will_occurences)\n",
    "df_kr_tf['shall'] = df_kospi_10k_1115['Translated'].dropna().str.count('shall')\n",
    "df_kr_tf['going_to'] = df_kospi_10k_1115['Translated'].dropna().str.count('going to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "will        7.084703\n",
       "shall       0.119014\n",
       "going_to    0.021444\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kr_tf[['will', 'shall', 'going_to']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sylim2357\\anaconda3\\envs\\future-tense-mining\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "price_df = pd.read_csv('D:\\\\data\\\\finance\\\\price\\\\prices_08_19.csv')\n",
    "stocks_of_interest = df_us_tf.gvkey.unique()\n",
    "price_df['datadate'] = pd.to_datetime(price_df['datadate'], format='%Y%m%d')\n",
    "price_df = price_df[price_df.gvkey.isin(stocks_of_interest)]\n",
    "price_df['adj_closing_price'] = price_df['prccd'] / price_df['ajexdi']\n",
    "price_df = price_df[['gvkey', 'datadate', 'tic', 'adj_closing_price']]\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>filed</th>\n",
       "      <th>will</th>\n",
       "      <th>shall</th>\n",
       "      <th>going_to</th>\n",
       "      <th>expect_to</th>\n",
       "      <th>expecting_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4843</td>\n",
       "      <td>2009-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2574</td>\n",
       "      <td>2009-04-15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12540</td>\n",
       "      <td>2009-06-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7881</td>\n",
       "      <td>2009-04-30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2991</td>\n",
       "      <td>2009-05-07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26415</th>\n",
       "      <td>34443</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26416</th>\n",
       "      <td>34765</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26417</th>\n",
       "      <td>35168</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26418</th>\n",
       "      <td>35035</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26419</th>\n",
       "      <td>11636</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26371 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gvkey      filed  will  shall  going_to  expect_to  expecting_to\n",
       "0       4843 2009-05-29   1.0    0.0       0.0        0.0           0.0\n",
       "1       2574 2009-04-15  11.0    0.0       0.0        0.0           0.0\n",
       "2      12540 2009-06-26   5.0    0.0       0.0        0.0           0.0\n",
       "4       7881 2009-04-30   5.0    0.0       0.0        1.0           0.0\n",
       "5       2991 2009-05-07   3.0    0.0       0.0        0.0           0.0\n",
       "...      ...        ...   ...    ...       ...        ...           ...\n",
       "26415  34443 2019-10-25  54.0    0.0       0.0        0.0           0.0\n",
       "26416  34765 2019-11-12  63.0    0.0       0.0        0.0           0.0\n",
       "26417  35168 2019-10-31  61.0    0.0       0.0        0.0           0.0\n",
       "26418  35035 2019-11-12  29.0    0.0       0.0        0.0           0.0\n",
       "26419  11636 2019-11-06  35.0    1.0       0.0        0.0           0.0\n",
       "\n",
       "[26371 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_gramm_index = df_us_tf[['gvkey', 'filed', 'will', 'shall', 'going_to', 'expect_to', 'expecting_to']].dropna()\n",
    "df_us_gramm_index.filed = pd.to_datetime(df_us_gramm_index.filed, format='%Y%m%d')\n",
    "df_us_gramm_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2008-01-01 00:00:00'),\n",
       " Timestamp('2009-01-20 00:00:00'),\n",
       " Timestamp('2009-04-20 00:00:00'),\n",
       " Timestamp('2009-07-20 00:00:00'),\n",
       " Timestamp('2009-10-20 00:00:00'),\n",
       " Timestamp('2010-01-20 00:00:00'),\n",
       " Timestamp('2010-04-20 00:00:00'),\n",
       " Timestamp('2010-07-20 00:00:00'),\n",
       " Timestamp('2010-10-20 00:00:00'),\n",
       " Timestamp('2011-01-20 00:00:00'),\n",
       " Timestamp('2011-04-20 00:00:00'),\n",
       " Timestamp('2011-07-20 00:00:00'),\n",
       " Timestamp('2011-10-20 00:00:00'),\n",
       " Timestamp('2012-01-20 00:00:00'),\n",
       " Timestamp('2012-04-20 00:00:00'),\n",
       " Timestamp('2012-07-20 00:00:00'),\n",
       " Timestamp('2012-10-20 00:00:00'),\n",
       " Timestamp('2013-01-20 00:00:00'),\n",
       " Timestamp('2013-04-20 00:00:00'),\n",
       " Timestamp('2013-07-20 00:00:00'),\n",
       " Timestamp('2013-10-20 00:00:00'),\n",
       " Timestamp('2014-01-20 00:00:00'),\n",
       " Timestamp('2014-04-20 00:00:00'),\n",
       " Timestamp('2014-07-20 00:00:00'),\n",
       " Timestamp('2014-10-20 00:00:00'),\n",
       " Timestamp('2015-01-20 00:00:00'),\n",
       " Timestamp('2015-04-20 00:00:00'),\n",
       " Timestamp('2015-07-20 00:00:00'),\n",
       " Timestamp('2015-10-20 00:00:00'),\n",
       " Timestamp('2016-01-20 00:00:00'),\n",
       " Timestamp('2016-04-20 00:00:00'),\n",
       " Timestamp('2016-07-20 00:00:00'),\n",
       " Timestamp('2016-10-20 00:00:00'),\n",
       " Timestamp('2017-01-20 00:00:00'),\n",
       " Timestamp('2017-04-20 00:00:00'),\n",
       " Timestamp('2017-07-20 00:00:00'),\n",
       " Timestamp('2017-10-20 00:00:00'),\n",
       " Timestamp('2018-01-20 00:00:00'),\n",
       " Timestamp('2018-04-20 00:00:00'),\n",
       " Timestamp('2018-07-20 00:00:00'),\n",
       " Timestamp('2018-10-20 00:00:00'),\n",
       " Timestamp('2019-01-20 00:00:00'),\n",
       " Timestamp('2019-04-20 00:00:00'),\n",
       " Timestamp('2019-07-20 00:00:00'),\n",
       " Timestamp('2019-10-20 00:00:00'),\n",
       " Timestamp('2020-01-20 00:00:00')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidq_dates_suffix_list = ['-01-20 00:00:00','-04-20 00:00:00','-07-20 00:00:00','-10-20 00:00:00']\n",
    "years = list(set([x.year for x in df_us_gramm_index['filed']]))\n",
    "years.sort()\n",
    "fidq_dates_list = [pd.Timestamp(str(years[0]-1))] + [pd.Timestamp(str(year)+suffix) for year in years for suffix in fidq_dates_suffix_list] + [pd.Timestamp(str(years[-1]+1)+fidq_dates_suffix_list[0])]\n",
    "fidq_dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_json('C:\\\\Users\\\\sylim2357\\\\Documents\\\\SNU\\\\Projects\\\\FVID\\\\data\\\\KRX\\\\kospi200_configs\\\\kospi_submission_date.json')\n",
    "df_sentiment = pd.read_csv('C:\\\\Users\\\\sylim2357\\\\Documents\\\\SNU\\\\Projects\\\\FVID\\\\data\\\\KRX\\\\kospi_sentiment\\\\kospi_sentiment_2011_2019.csv')\n",
    "market_days = pd.to_datetime(pd.read_csv('C:\\\\Users\\\\sylim2357\\\\Documents\\\\SNU\\\\Projects\\\\FVID\\\\data\\\\KRX\\\\date_normal.csv')['Date'])\n",
    "\n",
    "submission_df = submission_df[~submission_df['file_name'].str.contains(']')]\n",
    "submission_df['file_name'] = submission_df['file_name'].str.split('.pdf', expand=True).iloc[:,0]\n",
    "submission_df['company_code'] = submission_df['file_name'].str.split('_',expand=True).iloc[:,0]\n",
    "submission_df['submission_date'] = pd.to_datetime(submission_df['submission_date'], format='%Y%m%d')\n",
    "#constructing rebalancing dates\n",
    "fidq_dates_suffix_list = ['-01-20 00:00:00','-04-20 00:00:00','-07-20 00:00:00','-10-20 00:00:00']\n",
    "years = list(set([x.year for x in submission_df['submission_date']]))\n",
    "years.sort()\n",
    "fidq_dates_list = [pd.Timestamp(str(years[0]-1))] + [pd.Timestamp(str(year)+suffix) for year in years for suffix in fidq_dates_suffix_list] + [pd.Timestamp(str(years[-1]+1)+fidq_dates_suffix_list[0])]\n",
    "submission_df['rebalance_date'] = pd.cut(submission_df['submission_date'], fidq_dates_list, labels = fidq_dates_list[1:])\n",
    "#quantiling the sentiments per rebalance date\n",
    "sentiment_submission_df = submission_df.merge(df_sentiment, left_on='file_name', right_on='File').drop('File', axis=1)\n",
    "sentiment_submission_df['polarity_quantile'] = sentiment_submission_df.groupby('rebalance_date', level=0).apply(pd.DataFrame.sort_values, 'Polarity', ascending=False)['Polarity'].transform(lambda x: pd.qcut(x, 4, labels=range(1,5)))\n",
    "#filtering only top and bottom quantiles\n",
    "sentiment_submission_top_bottom_df = sentiment_submission_df[sentiment_submission_df.polarity_quantile.isin([1,4])]\n",
    "sentiment_submission_top_bottom_df['rebalance_date'] = pd.to_datetime(sentiment_submission_top_bottom_df['rebalance_date'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:future-tense-mining]",
   "language": "python",
   "name": "conda-env-future-tense-mining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
